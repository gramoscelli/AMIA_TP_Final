{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpJ7s_SIVu_I"
   },
   "source": [
    "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
    "\n",
    "### Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "### Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "### LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "### Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "### Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TDWOgpJWKQa"
   },
   "source": [
    "## Estructura del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEV8WbiWl6k"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "teF9O9JJmG7Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "sDBLvbTtlwzs"
   },
   "outputs": [],
   "source": [
    "class ClassEncoder:\n",
    "  def fit(self, y):\n",
    "    self.names = np.unique(y)\n",
    "    #self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
    "    self.fmt = y.dtype\n",
    "    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n",
    "    # Esto se hace con names. En y_hat tengo indices, que son los indices necesarios para recuperar nombres en names.\n",
    "    # De hecho, en names tengo nombres e índices. Podría usar \"names\" para transform y detransform (esto es lo que estoy haciendo)\n",
    "\n",
    "\n",
    "  def _map_reshape(self, f, arr):\n",
    "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
    "    # Q2: por que hace falta un reshape?\n",
    "    # Recupera la forma original de arr luego de aplicar la función f a cada elemento  \n",
    "\n",
    "  # La podría hacer privada para evitar que se invoque de afuera sin haber invocado a fit previamente\n",
    "  def transform(self, y):\n",
    "    #return self._map_reshape(lambda name: self.name_to_class[name], y) # No uso name_to_class!\n",
    "    return  self._map_reshape(lambda name: np.where(self.names == name)[0][0], y) # Acá np.where() devuleve (array([idx]),)\n",
    "\n",
    "  def fit_transform(self, y):\n",
    "    self.fit(y)\n",
    "    return self.transform(y)\n",
    "\n",
    "\n",
    "  # Puede pasar lo mismo que en tranform si no se llama a fit primero (no existe self.names)\n",
    "  def detransform(self, y_hat):\n",
    "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "m0KYC8_uSOu4"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    self.encoder = ClassEncoder()\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size # Calcula frecuencia de cada clase\n",
    "    # Q3: para que sirve bincount?\n",
    "    # Cuenta las ocurrencias de cada valor entero no negativo de y y devuelve un array\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "    # Method to estimate parameters, to be implemented in each child class based on the specific model.\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # first encode the classes\n",
    "    y = self.encoder.fit_transform(y)\n",
    "\n",
    "    # self._fit_params(X, y) # Eliminar esta linea, es sólo para probar Q4. Claramente rompe todo!\n",
    "\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # check that a_priori has the correct number of classes\n",
    "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1] # m_obs es igual a la cantidad de datos que se predicen\n",
    "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt) # Array de cadenas vacías, en principio. El tamano maximo de las cadenas\n",
    "    # está definido por fmt\n",
    "\n",
    "    # Recorre todos las observaciones buscando el máximo\n",
    "    for i in range(m_obs):\n",
    "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
    "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "    \n",
    "    # Defino atributos nuevos para analizar resultados\n",
    "    self.log_posteriori = log_posteriori\n",
    "    self.log_verosimilitud = [self._predict_log_conditional (x, idx) for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "IRamFdiGDuSR"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  # Estimo matrices de covarianza y medias\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "fRtC9HEkO5Hu"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means # Forma (classes, p, 1)\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        self.inner_prod = inner_prod # Forma (classes, 1, 1)\n",
    "\n",
    "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "    \n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "\n",
    "        classes = len(self.log_a_priori)\n",
    "        n = len(X[1])\n",
    "\n",
    "        # Forma de X: (p, n)\n",
    "        # Forma de self.tensor_means: (classes, p, 1)\n",
    "\n",
    "        X_expanded = np.expand_dims(X, axis=0)  # Forma (1, p, n)\n",
    "        X_expanded = np.tile(X_expanded, (classes, 1, 1))  # Forma (classes, p, n)\n",
    "\n",
    "        tensor_means_expanded = np.tile(self.tensor_means, (1, 1, n)) # Forma (classes, p, n)\n",
    "\n",
    "        unbiased_X = X_expanded - tensor_means_expanded # Forma (classes, p, n)\n",
    "\n",
    "        unbiased_X_transposed =  unbiased_X.transpose(0,2,1) # Forma (classes, n, p)\n",
    "\n",
    "        # Forma de self.tensor_inv_cov: (classes, p, p)\n",
    "        # \"classes\" matrices de nxn\n",
    "        inner_prod_mat = unbiased_X_transposed @ self.tensor_inv_cov @ unbiased_X # Forma (classes, n, n)\n",
    "\n",
    "        dets = np.linalg.det(self.tensor_inv_cov)  # Forma (classes,)\n",
    "        dets_expanded = np.expand_dims(dets, axis=1)  # Forma (classes, 1)\n",
    "        dets_expanded = np.tile(dets_expanded, (1, n))  # Forma (classes, n)\n",
    "\n",
    "        return 0.5 * np.log(dets_expanded) - 0.5 * np.diagonal(inner_prod_mat, axis1=1, axis2=2) # Forma (classes, n)\n",
    "    \n",
    "    def _predict_one(self, X):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        n = len(X[1])\n",
    "        log_a_priori_expanded = np.expand_dims(self.log_a_priori, axis=1)  # Forma (classes, 1)\n",
    "        log_a_priori_expanded = np.tile(log_a_priori_expanded, (1, n))  # Forma (classes, n)\n",
    "\n",
    "        self.log_a_posteriori = log_a_priori_expanded + self._predict_log_conditionals(X)\n",
    "\n",
    "        return np.argmax(self.log_a_posteriori, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction_expanded = np.expand_dims(self._predict_one(X), axis=0) # Expando a la forma [self._predict_one(X)]\n",
    "        # Convierto los indices de la predicción a nombres de clases (con detransform)\n",
    "        return self.encoder.detransform(prediction_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "\n",
    "        classes = len(self.log_a_priori)\n",
    "        n = len(X[1])\n",
    "\n",
    "        # Forma de X: (p, n)\n",
    "        # Forma de self.tensor_means: (classes, p, 1)\n",
    "\n",
    "        X_expanded = np.expand_dims(X, axis=0)  # Forma (1, p, n)\n",
    "        X_expanded = np.tile(X_expanded, (classes, 1, 1))  # Forma (classes, p, n)\n",
    "\n",
    "        tensor_means_expanded = np.tile(self.tensor_means, (1, 1, n)) # Forma (classes, p, n)\n",
    "\n",
    "        unbiased_X = X_expanded - tensor_means_expanded # Forma (classes, p, n)\n",
    "\n",
    "        # Forma de self.tensor_inv_cov: (classes, p, p)\n",
    "        unbiased_X_transposed =  unbiased_X.transpose(0,2,1) # Forma (classes, n, p)\n",
    "        A = unbiased_X_transposed @ self.tensor_inv_cov # Forma (classes, n, p)\n",
    "\n",
    "        # Siguiendo el enunciado, hago np.sum(A * B_tansposed, axis = 1), donde B es unbiesed_x\n",
    "        inner_prod = (A * unbiased_X_transposed).sum(axis=-1)  # Forma (classes, n)\n",
    "\n",
    "        dets = np.linalg.det(self.tensor_inv_cov)  # Forma (classes,)\n",
    "        dets_expanded = np.expand_dims(dets, axis=1)  # Forma (classes, 1)\n",
    "        dets_expanded = np.tile(dets_expanded, (1, n))  # Forma (classes, n)\n",
    "\n",
    "        return 0.5 * np.log(dets_expanded) - 0.5 * inner_prod # Forma (classes, n)\n",
    "\n",
    "    def _predict_one(self, X):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        n = len(X[1])\n",
    "        log_a_priori_expanded = np.expand_dims(self.log_a_priori, axis=1)  # Forma (classes, 1)\n",
    "        log_a_priori_expanded = np.tile(log_a_priori_expanded, (1, n))  # Forma (classes, n)\n",
    "\n",
    "        self.log_a_posteriori = log_a_priori_expanded + self._predict_log_conditionals(X)\n",
    "\n",
    "        return np.argmax(self.log_a_posteriori, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction_expanded = np.expand_dims(self._predict_one(X), axis=0) # Expando a la forma [self._predict_one(X)]\n",
    "        # Convierto los indices de la predicción a nombres de clases (con detransform)\n",
    "        return self.encoder.detransform(prediction_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    covs =  [(np.cov(X[:,y.flatten()==idx], bias=True)) for idx in range(len(self.log_a_priori))]\n",
    "    log_a_priori = self.log_a_priori\n",
    "    self.inv_covs =  [inv(sum([val * mat for val, mat in zip(np.exp(log_a_priori), covs)]))]\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[0]\n",
    "    modified_x =  x - 0.5*self.means[class_idx] # Ver nombre! Esto es (x-0.5 muj)\n",
    "    return self.means[class_idx].T @ inv_cov @ modified_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedLDA(LDA):\n",
    "  def _fit_params(self, X, y):\n",
    "    # ask plain QDA to fit params\n",
    "    super()._fit_params(X,y)\n",
    "\n",
    "    # stack onto new dimension\n",
    "    self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "    self.tensor_means = np.stack(self.means)\n",
    "\n",
    "  def _predict_log_conditionals(self, x):\n",
    "    modified_x =  x - 0.5*self.tensor_means\n",
    "    result = self.tensor_means.transpose(0,2,1) @ self.tensor_inv_cov @ modified_x\n",
    "    return result.flatten()\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "      # return the class that has maximum a posteriori probability\n",
    "      return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterLDA(TensorizedLDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "\n",
    "        classes = len(self.log_a_priori)\n",
    "        n = len(X[1])\n",
    "\n",
    "        X_expanded = np.expand_dims(X, axis=0)  # Forma (1, p, n)\n",
    "        X_expanded = np.tile(X_expanded, (classes, 1, 1))  # Forma (classes, p, n)\n",
    "\n",
    "        tensor_means_expanded = np.tile(self.tensor_means, (1, 1, n)) # Forma (classes, p, n)\n",
    "        modified_X = X_expanded - 0.5*tensor_means_expanded # Forma (classes, p, n)\n",
    "\n",
    "        #return self.tensor_means.transpose(0,2,1) @ self.tensor_inv_cov @ modified_X # Más directo pero un poco más lento que de la siguiente forma.\n",
    "\n",
    "        # Repitiendo lo hecho en FasterQDA()\n",
    "        modified_X_transposed =  modified_X.transpose(0,2,1) # Forma (classes, n, p)\n",
    "        tensor_means_expanded_transposed = tensor_means_expanded.transpose(0,2,1)\n",
    "        A = tensor_means_expanded_transposed @ self.tensor_inv_cov  # Forma (classes, n, p)\n",
    "\n",
    "        return (A * modified_X_transposed).sum(axis=-1) # Forma (3, n)\n",
    "\n",
    "    def _predict_one(self, X):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        n = len(X[1])\n",
    "        log_a_priori_expanded = np.expand_dims(self.log_a_priori, axis=1)  # Forma (3, 1)\n",
    "        log_a_priori_expanded = np.tile(log_a_priori_expanded, (1, n))  # Forma (3, n)\n",
    "\n",
    "        self.log_a_posteriori = log_a_priori_expanded + self._predict_log_conditionals(X)\n",
    "\n",
    "        return np.argmax(self.log_a_posteriori, axis=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction_expanded = np.expand_dims(self._predict_one(X), axis=0) # Expando a la forma [self._predict_one(X)]\n",
    "        # Convierto los indices de la predicción a nombres de clases (con detransform)\n",
    "        return self.encoder.detransform(prediction_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS_zoK-gWkRf"
   },
   "source": [
    "## Código para pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz19b6NJed2A"
   },
   "source": [
    "Seteamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "m05KrhUDINVs"
   },
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 6543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hkXcoldXOqs",
    "outputId": "2ce8d627-3433-4bdd-d370-85f6b703a7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (150, 4), Y:(150, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto') \n",
    "    # Vector con las etiquetas, que en este caso corresponden a las especies de pinguinos, asociadas a cada fila del DataFrame df\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "# showing for iris\n",
    "X_full, y_full = get_iris_dataset()\n",
    "\n",
    "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAk-UQCjKecT",
    "outputId": "9566d67a-b78b-4809-bb94-8f605b065db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek data matrix\n",
    "X_full[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdzMURX2KVdO",
    "outputId": "af5fc3ac-b391-4769-de47-44cea4f566c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa']], dtype='<U10')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek target vector\n",
    "y_full[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl8UFh1OegbJ"
   },
   "source": [
    "Separamos el dataset en train y test para medir performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKP_QmWCIECs",
    "outputId": "07798c6a-aa54-430e-d46d-becc2a4315ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 90) (1, 90) (4, 60) (1, 60)\n"
     ]
    }
   ],
   "source": [
    "# preparing data, train - test validation\n",
    "# 70-30 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_transpose(X, y, test_sz, random_state):\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sz, random_state=random_state)\n",
    "\n",
    "    # transpose so observations are column vectors\n",
    "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "\n",
    "# Ver distribución de clases\n",
    "print(data['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwgXFPbJemb_"
   },
   "source": [
    "Entrenamos un QDA y medimos su accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "dGIf2TA5SpoT"
   },
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "lda = LDA()\n",
    "\n",
    "qda.fit(train_x, train_y)\n",
    "lda.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Q30DyLWpTL",
    "outputId": "dbccae86-840c-412f-ed97-22cfac21238a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QcLtNNIevC_"
   },
   "source": [
    "Con el magic %%timeit podemos estimar el tiempo que tarda en correr una celda en base a varias ejecuciones. Por poner un ejemplo, acá vamos a estimar lo que tarda un ciclo completo de QDA y también su inferencia (predicción).\n",
    "\n",
    "Ojo! a veces [puede ser necesario ejecutarlo varias veces](https://stackoverflow.com/questions/10994405/python-timeit-results-cached-instead-of-calculated) para obtener resultados consistentes.\n",
    "\n",
    "Si quieren explorar otros métodos de medición también es válido!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnZT-HN2fUuW",
    "outputId": "2618e7c1-7a77-4285-bafb-c2880ad167a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.52 ms ± 562 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjFbVSqfeHUX",
    "outputId": "0254a727-a1d5-4be3-b73a-2f55d2c84a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.88 ms ± 592 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model = QDA()\n",
    "model.fit(train_x, train_y)\n",
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDYvwFEEKV5"
   },
   "source": [
    "# Consigna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yb1V7_yXRfO"
   },
   "source": [
    "## Implementación base\n",
    "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
    "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
    "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "2. Repetir el punto anterior para el dataset *penguin*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución\n",
    "\n",
    "## Ejercicio 1 y 2\n",
    "\n",
    "En el siguiente código, comente la línea correspondiente para realizar el ejercicio 1 o 2.\n",
    "\n",
    "La clasificación entre una clase y otra depende de dos términos: $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$. Para una predicción específica, la elección de una clase sobre otra, cuando el primer término es fijo (ya que depende del conjunto de entrenamiento y de la observación a predecir, siendo esta igual para todas las clases), puede definirse en función del valor del logaritmo de la probabilidad a priori (básicamente la proporción de cada clase conocida de antemano).\n",
    "\n",
    "En particular, para un problema dado, ambos términos tienen un peso relativo, y el valor del logaritmo de la probabilidad a priori puede ser decisivo al determinar si una observación pertenece a una clase u otra. Es posible, además, que conjuntos diferentes de probabilidades condicionales produzcan las mismas predicciones.\n",
    "\n",
    "En determinadas comparaciones, cuando la diferencia entre los logaritmos de la verosimilitud entre clases no es suficientemente grande, la probabilidad a priori puede inclinar la balanza a favor de una clase. Por otro lado, si la diferencia entre los logartimos de las verosimilitudes para las distintas clases es significativa, el logaritmo de la probabilidad a priori pierde relevancia y no resulta decisivo.\n",
    "\n",
    "En el ejemplo de las flores, cambiar la probabilidad a priori de $[1/3, 1/3, 1/3]$ a $[0.9, 0.05, 0.05]$ (asociado a las clases [*setosa*, *versicolor*, *virginica*]) no afecta las decisiones del clasificador, como lo demuestra la precisión obtenida en el código. Esto ocurre porque se mantiene la relación de probabilidades a priori entre las flores con características menos diferenciadas, es decir, *versicolor* y *virginica*, lo que implica que el clasificador no muestra preferencia por una de estas clases sobre la otra. Al comparar estas clases de flores con la clase *setosa*, puede verse que este cambio particular en las probabilidades a priori no altera el resultado debido al \"peso\" considerable del logaritmo de la verosimilitud de esta última clase. **En todos los casos analizados, la clase con la máxima probabilidad a posteriori coincide con la que presenta la máxima verosimilitud.**\n",
    "\n",
    "En contraste, si las probabilidades a priori cambian para clases con características más parecidas, por ejemplo, pasar de $[0.05, 0.9, 0.05]$ a $[0.05, 0.05, 0.9]$ (o de $[1/3, 1/3, 1/3]$ a $[0.05, 0.9, 0.05]$ o a $[0.05, 0.05, 0.9]$) sí puede afectar las predicciones. En este caso, el logaritmo de las probabilidades a priori influye en las decisiones del clasificador para ciertas observaciones específicas. Por este motivo, la precisión de la predicción sobre el set de prueba varía entre un caso y otro.\n",
    "\n",
    "En el caso de los pingüinos, las dos clases con características más parecidas son *Adelie* y *Chinstrap*. Considerar diferentes probabilidades a priori para ambas (por ejemplo, una con una probabilidad de $0.9$ y la otra de $0.05$) puede llevar a que el clasificador escoja, para algunas observaciones particulares, un tipo que no coincide con el real, o al menos con el que el clasificador habría seleccionado si las probabilidades a priori entre estas clases fueran diferentes.\n",
    "\n",
    "En este ejercicio en particular, mantener la probabilidad a priori relativa entre las clases más parecidas sin cambios, es decir, pasar de la probabilidad a priori $[1/3, 1/3, 1/3]$ a $[0.05, 0.05, 0.9]$ (asociado a las clases [*Adelie*, *Chinstrap*, *Gentoo*]), no afecta la precisión de las predicciones.\n",
    "\n",
    "Para este analisis se consideraron `n_runs` ejecuciones y un **split** aleatorio para cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión con priori = [1/3, 1/3, 1/3]: 0.99123\n",
      "Precisión con priori = [0.9, 0.05, 0.05]: 0.98246\n",
      "Precisión con priori = [0.05, 0.9, 0.05]: 0.96491\n",
      "Precisión con priori = [0.05, 0.05, 0.9]: 0.99123\n",
      "-------------------------\n",
      "priori = [1/3, 1/3, 1/3]\n",
      "Clase Adelie - log verosimilitud, log verosimilitud + log priori: -34.43, -35.53\n",
      "Clase Chinstrap - log verosimilitud, log verosimilitud + log priori: -36.63, -37.73\n",
      "Clase Gentoo - log verosimilitud, log verosimilitud + log priori: -11.26, -12.36\n",
      "priori = [0.9, 0.05, 0.05]\n",
      "Clase Adelie - log verosimilitud, log verosimilitud + log priori: -34.43, -34.54\n",
      "Clase Chinstrap - log verosimilitud, log verosimilitud + log priori: -36.63, -39.63\n",
      "Clase Gentoo - log verosimilitud, log verosimilitud + log priori: -11.26, -14.25\n",
      "priori = [0.05, 0.9, 0.05]\n",
      "Clase Adelie - log verosimilitud, log verosimilitud + log priori: -34.43, -37.43\n",
      "Clase Chinstrap - log verosimilitud, log verosimilitud + log priori: -36.63, -36.74\n",
      "Clase Gentoo - log verosimilitud, log verosimilitud + log priori: -11.26, -14.25\n",
      "priori = [0.05, 0.05, 0.9]\n",
      "Clase Adelie - log verosimilitud, log verosimilitud + log priori: -34.43, -37.43\n",
      "Clase Chinstrap - log verosimilitud, log verosimilitud + log priori: -36.63, -39.63\n",
      "Clase Gentoo - log verosimilitud, log verosimilitud + log priori: -11.26, -11.36\n"
     ]
    }
   ],
   "source": [
    "# Comentar la linea correspondiente para ejercicio 1 o 2.\n",
    "#X_full, y_full = get_iris_dataset() # Ejercicio 1\n",
    "X_full, y_full = get_penguins() # Ejercicio 2\n",
    "\n",
    "# Defino un modelo para cada probabilidad a priori\n",
    "qda_priori_1, priori_1 = QDA(), np.array([1/3, 1/3, 1/3])\n",
    "qda_priori_2, priori_2 = QDA(), np.array([0.9, 0.05, 0.05])\n",
    "qda_priori_3, priori_3 = QDA(), np.array([0.05, 0.9, 0.05])\n",
    "qda_priori_4, priori_4 = QDA(), np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "# Número de ejecuciones aleatorias\n",
    "n_runs = 40\n",
    "\n",
    "# Defino listas que uso spara informar resultados\n",
    "list_acc_1 = []\n",
    "list_acc_2 = []\n",
    "list_acc_3 = []\n",
    "list_acc_4 = []\n",
    "\n",
    "list_log_verosimilitud = [[], [], []]\n",
    "\n",
    "# Ejecutar múltiples corridas\n",
    "for run in range(n_runs):\n",
    "    # Dividir los datos de forma aleatoria\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=None)\n",
    "    #train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=rng_seed)\n",
    "\n",
    "    train_x = X_full.T\n",
    "    train_y = y_full.T\n",
    "    test_x = X_full.T\n",
    "    test_y = y_full.T\n",
    "\n",
    "    qda_priori_1.fit(train_x, train_y, priori_1)\n",
    "    qda_priori_2.fit(train_x, train_y, priori_2)\n",
    "    qda_priori_3.fit(train_x, train_y, priori_3)\n",
    "    qda_priori_4.fit(train_x, train_y, priori_4)\n",
    "\n",
    "    list_acc_1.append(accuracy(test_y, qda_priori_1.predict(test_x)))\n",
    "    list_acc_2.append(accuracy(test_y, qda_priori_2.predict(test_x)))\n",
    "    list_acc_3.append(accuracy(test_y, qda_priori_3.predict(test_x)))\n",
    "    list_acc_4.append(accuracy(test_y, qda_priori_4.predict(test_x)))\n",
    "\n",
    "    log_verosimilitud = qda_priori_1.log_verosimilitud\n",
    "    for idx, val in enumerate(log_verosimilitud):\n",
    "        list_log_verosimilitud[idx].append(val[0][0])\n",
    "\n",
    "\n",
    "print(\"Precisión con priori = [1/3, 1/3, 1/3]: {:.5f}\".format(np.mean(list_acc_1)))\n",
    "print(\"Precisión con priori = [0.9, 0.05, 0.05]: {:.5f}\".format(np.mean(list_acc_2)))\n",
    "print(\"Precisión con priori = [0.05, 0.9, 0.05]: {:.5f}\".format(np.mean(list_acc_3)))\n",
    "print(\"Precisión con priori = [0.05, 0.05, 0.9]: {:.5f}\".format(np.mean(list_acc_4)))\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"priori = [1/3, 1/3, 1/3]\")\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_1.encoder.names[0], np.mean(list_log_verosimilitud[0]), np.mean(list_log_verosimilitud[0]) + np.log(priori_1[0])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_1.encoder.names[1], np.mean(list_log_verosimilitud[1]), np.mean(list_log_verosimilitud[1]) + np.log(priori_1[1])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_1.encoder.names[2], np.mean(list_log_verosimilitud[2]), np.mean(list_log_verosimilitud[2]) + np.log(priori_1[2])))\n",
    "\n",
    "print(\"priori = [0.9, 0.05, 0.05]\")\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_2.encoder.names[0], np.mean(list_log_verosimilitud[0]), np.mean(list_log_verosimilitud[0]) + np.log(priori_2[0])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_2.encoder.names[1], np.mean(list_log_verosimilitud[1]), np.mean(list_log_verosimilitud[1]) + np.log(priori_2[1])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_2.encoder.names[2], np.mean(list_log_verosimilitud[2]), np.mean(list_log_verosimilitud[2]) + np.log(priori_2[2])))\n",
    "\n",
    "print(\"priori = [0.05, 0.9, 0.05]\")\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_3.encoder.names[0], np.mean(list_log_verosimilitud[0]), np.mean(list_log_verosimilitud[0]) + np.log(priori_3[0])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_3.encoder.names[1], np.mean(list_log_verosimilitud[1]), np.mean(list_log_verosimilitud[1]) + np.log(priori_3[1])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_3.encoder.names[2], np.mean(list_log_verosimilitud[2]), np.mean(list_log_verosimilitud[2]) + np.log(priori_3[2])))\n",
    "\n",
    "print(\"priori = [0.05, 0.05, 0.9]\")\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_4.encoder.names[0], np.mean(list_log_verosimilitud[0]), np.mean(list_log_verosimilitud[0]) + np.log(priori_4[0])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_4.encoder.names[1], np.mean(list_log_verosimilitud[1]), np.mean(list_log_verosimilitud[1]) + np.log(priori_4[1])))\n",
    "print(\"Clase {} - log verosimilitud, log verosimilitud + log priori: {:.2f}, {:.2f}\".format(qda_priori_4.encoder.names[2], np.mean(list_log_verosimilitud[2]), np.mean(list_log_verosimilitud[2]) + np.log(priori_4[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo algunas métricas más (de sklearn). Uso el mismo bucle *for* que en la celda anterior. **Este análisis es sólo para intentar inferir más características del comportamiento del modelo en base a las diferentes probaiblidades a priori.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc., Acc. (sklearn), Pre. y Rec. con priori = [1/3, 1/3, 1/3]: 0.97278, 0.97278, 0.97368, 0.97262\n",
      "Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.9, 0.05, 0.05]: 0.97278, 0.97278, 0.97368, 0.97262\n",
      "Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.05, 0.9, 0.05]: 0.95278, 0.95278, 0.95666, 0.95209\n",
      "Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.05, 0.05, 0.9]: 0.95667, 0.95667, 0.96225, 0.95518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Comentar la linea correspondiente para ejercicio 1 o 2.\n",
    "X_full, y_full = get_iris_dataset() # Ejercicio 1\n",
    "#X_full, y_full = get_penguins() # Ejercicio 2\n",
    "\n",
    "# Defino un modelo para cada probabilidad a priori\n",
    "qda_priori_1, priori_1 = QDA(), np.array([1/3, 1/3, 1/3])\n",
    "qda_priori_2, priori_2 = QDA(), np.array([0.9, 0.05, 0.05])\n",
    "qda_priori_3, priori_3 = QDA(), np.array([0.05, 0.9, 0.05])\n",
    "qda_priori_4, priori_4 = QDA(), np.array([0.05, 0.05, 0.9])\n",
    "\n",
    "# Número de ejecuciones aleatorias\n",
    "n_runs = 40\n",
    "\n",
    "# Defino listas que uso spara informar resultados\n",
    "list_acc_1, Accuracy_1, Precision_1, Recall_1 = [], [], [], []\n",
    "list_acc_2, Accuracy_2, Precision_2, Recall_2 = [], [], [], []\n",
    "list_acc_3, Accuracy_3, Precision_3, Recall_3 = [], [], [], []\n",
    "list_acc_4, Accuracy_4, Precision_4, Recall_4 = [], [], [], []\n",
    "\n",
    "list_verosimilitud = [[], [], []]\n",
    "\n",
    "# Ejecutar múltiples corridas\n",
    "for run in range(n_runs):\n",
    "    # Dividir los datos de forma aleatoria\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=None)\n",
    "    #train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=rng_seed)\n",
    "    \n",
    "    # Estas dos lineas son sólo para evaluar qué sucede si analizamos el rendimiento de los modelos sobre el set completo\n",
    "    #test_x = X_full.T \n",
    "    #test_y = y_full.T\n",
    "\n",
    "    qda_priori_1.fit(train_x, train_y, priori_1)\n",
    "    qda_priori_2.fit(train_x, train_y, priori_2)\n",
    "    qda_priori_3.fit(train_x, train_y, priori_3)\n",
    "    qda_priori_4.fit(train_x, train_y, priori_4)\n",
    "\n",
    "    qda_priori_1_predict = qda_priori_1.predict(test_x)\n",
    "    qda_priori_2_predict = qda_priori_2.predict(test_x)\n",
    "    qda_priori_3_predict = qda_priori_3.predict(test_x)\n",
    "    qda_priori_4_predict = qda_priori_4.predict(test_x)\n",
    "\n",
    "    # Accuracy usando función propia\n",
    "    list_acc_1.append(accuracy(test_y, qda_priori_1_predict))\n",
    "    list_acc_2.append(accuracy(test_y, qda_priori_2_predict))\n",
    "    list_acc_3.append(accuracy(test_y, qda_priori_3_predict))\n",
    "    list_acc_4.append(accuracy(test_y, qda_priori_4_predict)) \n",
    "\n",
    "    # Accuracy usando sklearn\n",
    "    Accuracy_1.append(accuracy_score(test_y[0], qda_priori_1_predict[0]))\n",
    "    Accuracy_2.append(accuracy_score(test_y[0], qda_priori_2_predict[0]))\n",
    "    Accuracy_3.append(accuracy_score(test_y[0], qda_priori_3_predict[0]))\n",
    "    Accuracy_4.append(accuracy_score(test_y[0], qda_priori_4_predict[0]))\n",
    "\n",
    "    Precision_1.append(precision_score(test_y[0], qda_priori_1_predict[0], average='macro'))\n",
    "    Precision_2.append(precision_score(test_y[0], qda_priori_2_predict[0], average='macro'))\n",
    "    Precision_3.append(precision_score(test_y[0], qda_priori_3_predict[0], average='macro'))\n",
    "    Precision_4.append(precision_score(test_y[0], qda_priori_4_predict[0], average='macro'))\n",
    "\n",
    "    Recall_1.append(recall_score(test_y[0], qda_priori_1_predict[0], average='macro'))\n",
    "    Recall_2.append(recall_score(test_y[0], qda_priori_2_predict[0], average='macro'))\n",
    "    Recall_3.append(recall_score(test_y[0], qda_priori_3_predict[0], average='macro'))\n",
    "    Recall_4.append(recall_score(test_y[0], qda_priori_4_predict[0], average='macro'))\n",
    "\n",
    "print(f\"Acc., Acc. (sklearn), Pre. y Rec. con priori = [1/3, 1/3, 1/3]: \"\n",
    "      f\"{np.mean(list_acc_1):.5f}, \"\n",
    "      f\"{np.mean(Accuracy_1):.5f}, \"\n",
    "      f\"{np.mean(Precision_1):.5f}, \"\n",
    "      f\"{np.mean(Recall_1):.5f}\")\n",
    "\n",
    "print(f\"Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.9, 0.05, 0.05]: \"\n",
    "      f\"{np.mean(list_acc_2):.5f}, \"\n",
    "      f\"{np.mean(Accuracy_2):.5f}, \"\n",
    "      f\"{np.mean(Precision_2):.5f}, \"\n",
    "      f\"{np.mean(Recall_2):.5f}\")\n",
    "\n",
    "print(f\"Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.05, 0.9, 0.05]: \"\n",
    "      f\"{np.mean(list_acc_3):.5f}, \"\n",
    "      f\"{np.mean(Accuracy_3):.5f}, \"\n",
    "      f\"{np.mean(Precision_3):.5f}, \"\n",
    "      f\"{np.mean(Recall_3):.5f}\")\n",
    "\n",
    "print(f\"Acc., Acc. (sklearn), Pre. y Rec. con priori = [0.05, 0.05, 0.9]: \"\n",
    "      f\"{np.mean(list_acc_4):.5f}, \"\n",
    "      f\"{np.mean(Accuracy_4):.5f}, \"\n",
    "      f\"{np.mean(Precision_4):.5f}, \"\n",
    "      f\"{np.mean(Recall_4):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación base\n",
    "\n",
    "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
    "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 y 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priori: [0.33333333 0.33333333 0.33333333]\n",
      "QDA acc: 0.9918 (std: 0.0079)\n",
      "LDA acc: 0.9920 (std: 0.0079)\n",
      "Priori: [0.05 0.9  0.05]\n",
      "QDA acc: 0.9781 (std: 0.0219)\n",
      "LDA acc: 0.9749 (std: 0.0234)\n",
      "Priori: [0.05 0.05 0.9 ]\n",
      "QDA acc: 0.9816 (std: 0.0192)\n",
      "LDA acc: 0.9806 (std: 0.0213)\n"
     ]
    }
   ],
   "source": [
    "#X_full, y_full = get_iris_dataset() # Ejercicio 1\n",
    "X_full, y_full = get_penguins() # Ejercicio 2\n",
    "\n",
    "\n",
    "# Defino un modelo para cada probabilidad a priori\n",
    "priori_arr = np.array([[1/3, 1/3, 1/3], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "qda = QDA()\n",
    "lda = LDA()\n",
    "\n",
    "# Número de corridas aleatorias\n",
    "n_runs = 50\n",
    "\n",
    "# Defino listas que uso spara informar resultados\n",
    "list_test_acc_qda = []\n",
    "list_test_acc_lda = []\n",
    "\n",
    "# Ejecutar múltiples corridas\n",
    "\n",
    "for priori in priori_arr:\n",
    "    for run in range(n_runs):\n",
    "        # Dividir los datos de forma aleatoria\n",
    "        train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=None)\n",
    "        #train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=rng_seed)\n",
    "\n",
    "        qda.fit(train_x, train_y, priori)\n",
    "        lda.fit(train_x, train_y, priori)\n",
    "\n",
    "        list_test_acc_qda.append(accuracy(test_y, qda.predict(test_x)))\n",
    "        list_test_acc_lda.append(accuracy(test_y, lda.predict(test_x)))\n",
    "\n",
    "    print(\"Priori:\", priori)\n",
    "    print(\"QDA acc: {:.4f} (std: {:.4f})\".format(np.mean(list_test_acc_qda), np.std(list_test_acc_qda)))\n",
    "    print(\"LDA acc: {:.4f} (std: {:.4f})\".format(np.mean(list_test_acc_lda), np.std(list_test_acc_lda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principales resultados: \n",
    "\n",
    "<center>\n",
    "\n",
    "Prior: [0.333333, 0.333333, 0.333333]\n",
    "\n",
    "Modelo | Accuracy | Std |\n",
    ":---: | :---: | :---: |\n",
    "QDA | 0.9886  | 0.0069 |\n",
    "LDA | 0.9901 | 0.0056 |\n",
    "\n",
    "Prior: [0.05, 0.9, 0.05]\n",
    "\n",
    "Modelo | Accuracy | Std |\n",
    ":---: | :---: | :---: |\n",
    "QDA | 0.9715  | 0.0255 |\n",
    "LDA | 0.9666 | 0.0295 |\n",
    "\n",
    "Prior: [0.05, 0.05, 0.9]\n",
    "\n",
    "Modelo | Accuracy | Std |\n",
    ":---: | :---: | :---: |\n",
    "QDA | 0.9770  | 0.0227 |\n",
    "LDA | 0.9751 | 0.0272 |\n",
    "\n",
    "</center>\n",
    "\n",
    " * Alta precisión en ambos modelos.\n",
    " * Variaciones mínimas en las distribuciónes para las diferentes prioridades a priori consideradas.\n",
    " * Basado en los resultados (considerando `n_runs` ejecuciones) y utilizando tres probabilidades a priori diferentes (para tener un análisis más completo), no se puede afirmar de manera concluyente que un modelo sea mejor que otro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación base\n",
    "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n",
    "\n",
    "**Sugerencia:** puede resultar de utilidad para cada inciso de comparación utilizar tablas del siguiente estilo:\n",
    "\n",
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 125 | 0.55 | 0.85\n",
    "LDA | Iris | 125 | 0.22 | 0.8\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA acc: 0.988 (std: 0.009)\n",
      "Tensorized QDA acc: 0.988 (std: 0.009)\n",
      "QDA time: 0.0486 (std: 0.0076)\n",
      "Tensorized QDA time: 0.0105 (std: 0.0034)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "#X_full, y_full = get_iris_dataset() # Ejercicio 1\n",
    "X_full, y_full = get_penguins() # Ejercicio 2\n",
    "\n",
    "# Defino un modelo para cada probabilidad a priori\n",
    "priori_arr = np.array([[1/3, 1/3, 1/3], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "qda = QDA()\n",
    "qda_T = TensorizedQDA()\n",
    "\n",
    "# Número de corridas aleatorias\n",
    "n_runs = 100\n",
    "\n",
    "# Defino listas que uso spara informar resultados\n",
    "list_test_acc_qda = []\n",
    "list_test_acc_qda_T = []\n",
    "list_test_time_qda = []\n",
    "list_test_time_qda_T = []\n",
    "\n",
    "# Ejecutar múltiples corridas\n",
    "\n",
    "for priori in priori_arr[:1]:\n",
    "    for run in range(n_runs):\n",
    "        # Dividir los datos de forma aleatoria\n",
    "        train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=None)\n",
    "        #train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=rng_seed)\n",
    "\n",
    "        qda.fit(train_x, train_y, priori)\n",
    "        qda_T.fit(train_x, train_y, priori)\n",
    "\n",
    "        # Calcular tiempo y guardar en listas\n",
    "        start_time = time()\n",
    "        list_test_acc_qda.append(accuracy(test_y, qda.predict(test_x)))\n",
    "        list_test_time_qda.append(time() - start_time)\n",
    "\n",
    "        start_time = time()\n",
    "        list_test_acc_qda_T.append(accuracy(test_y, qda_T.predict(test_x)))\n",
    "        list_test_time_qda_T.append(time() - start_time)\n",
    "\n",
    "    print(\"QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda), np.std(list_test_acc_qda)))\n",
    "    print(\"Tensorized QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda_T), np.std(list_test_acc_qda_T)))\n",
    "\n",
    "    print(\"QDA time: {:.4f} (std: {:.4f})\".format(np.mean(list_test_time_qda), np.std(list_test_time_qda)))\n",
    "    print(\"Tensorized QDA time: {:.4f} (std: {:.4f})\".format(np.mean(list_test_time_qda_T), np.std(list_test_time_qda_T)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principales resultados:\n",
    "\n",
    "<center>\n",
    "\n",
    "Modelo | Accuracy | std (acc) | Tiempo | std (t) |\n",
    ":---: | :---: | :---: | :---: | :---: | \n",
    "QDA | 0.988  | 0.009 | 0.0189 | 0.0116 | \n",
    "Tensorized QDA | 0.988 | 0.009 | 0.0038 | 0.0024 |\n",
    "\n",
    "</center>\n",
    "\n",
    " * Ambos modelos mantienen la misma precisión\n",
    " * TQDA es más rápido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M32U5xI-gIv"
   },
   "source": [
    "## Optimización matemática\n",
    "\n",
    "**Sugerencia:** considerar combinaciones adecuadas de `transpose`, `reshape` y, ocasionalmente, `flatten`. Explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "### QDA\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
    "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
    "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
    "4.Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
    "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?\n",
    "\n",
    "\n",
    "### LDA\n",
    "\n",
    "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
    "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización matemática - Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA acc: 0.973 (std: 0.022)\n",
      "Tensorized QDA acc: 0.973 (std: 0.022)\n",
      "faster QDA acc: 0.973 (std: 0.022)\n",
      "Faster QDA acc: 0.973 (std: 0.022)\n",
      "LDA acc: 0.978 (std: 0.019)\n",
      "Tensorized LDA acc: 0.978 (std: 0.019)\n",
      "Faster LDA acc: 0.978 (std: 0.019)\n",
      "QDA time: 0.01863 (std: 0.00474)\n",
      "Tensorized QDA time: 0.00420 (std: 0.00235)\n",
      "faster QDA time: 0.00066 (std: 0.00151)\n",
      "Faster QDA time: 0.00052 (std: 0.00129)\n",
      "LDA time: 0.00910 (std: 0.00331)\n",
      "Tensorized LDA time: 0.00276 (std: 0.00230)\n",
      "Faster LDA time: 0.00048 (std: 0.00129)\n"
     ]
    }
   ],
   "source": [
    "X_full, y_full = get_iris_dataset() \n",
    "\n",
    "# Defino un modelo para cada probabilidad a priori\n",
    "priori = [1/3, 1/3, 1/3]\n",
    "qda = QDA()\n",
    "qda_T = TensorizedQDA()\n",
    "qda_f = fasterQDA()\n",
    "qda_F = FasterQDA()\n",
    "lda = LDA()\n",
    "lda_T = TensorizedLDA()\n",
    "lda_F = FasterLDA()\n",
    "\n",
    "# Número de corridas aleatorias\n",
    "n_runs = 1000\n",
    "\n",
    "# Defino listas que uso para informar resultados\n",
    "list_test_acc_qda = []\n",
    "list_test_acc_qda_T = []\n",
    "list_test_acc_qda_f = []\n",
    "list_test_acc_qda_F = []\n",
    "list_test_acc_lda = []\n",
    "list_test_acc_lda_T = []\n",
    "list_test_acc_lda_F = []\n",
    "\n",
    "list_test_time_qda = []\n",
    "list_test_time_qda_T = []\n",
    "list_test_time_qda_f = []\n",
    "list_test_time_qda_F = []\n",
    "list_test_time_lda = []\n",
    "list_test_time_lda_T = []\n",
    "list_test_time_lda_F = []\n",
    "\n",
    "# Ejecutar múltiples corridas\n",
    "\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Dividir los datos de forma aleatoria\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, random_state=None)\n",
    "\n",
    "    # test_x = X_full.T # Para analizar performance en tiempo\n",
    "    # test_y = y_full.T # Para analizar performance en tiempo\n",
    "    \n",
    "\n",
    "    qda.fit(train_x, train_y, priori)\n",
    "    qda_T.fit(train_x, train_y, priori)\n",
    "    qda_f.fit(train_x, train_y, priori)\n",
    "    qda_F.fit(train_x, train_y, priori)\n",
    "    lda.fit(train_x, train_y, priori)\n",
    "    lda_T.fit(train_x, train_y, priori)\n",
    "    lda_F.fit(train_x, train_y, priori)\n",
    "\n",
    "    # Calcular tiempo y guardar en listas\n",
    "    start_time = time()\n",
    "    list_test_acc_qda.append(accuracy(test_y, qda.predict(test_x)))\n",
    "    list_test_time_qda.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_qda_T.append(accuracy(test_y, qda_T.predict(test_x)))\n",
    "    list_test_time_qda_T.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_qda_f.append(accuracy(test_y, qda_f.predict(test_x)))\n",
    "    list_test_time_qda_f.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_qda_F.append(accuracy(test_y, qda_F.predict(test_x)))\n",
    "    list_test_time_qda_F.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_lda.append(accuracy(test_y, lda.predict(test_x)))\n",
    "    list_test_time_lda.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_lda_T.append(accuracy(test_y, lda_T.predict(test_x)))\n",
    "    list_test_time_lda_T.append(time() - start_time)\n",
    "\n",
    "    start_time = time()\n",
    "    list_test_acc_lda_F.append(accuracy(test_y, lda_F.predict(test_x)))\n",
    "    list_test_time_lda_F.append(time() - start_time)\n",
    "\n",
    "\n",
    "print(\"QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda), np.std(list_test_acc_qda)))\n",
    "print(\"Tensorized QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda_T), np.std(list_test_acc_qda_T)))\n",
    "print(\"faster QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda_f), np.std(list_test_acc_qda_f))) # Pasando por la matriz nxn (en realidad por el tensor classes x nxn)\n",
    "print(\"Faster QDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_qda_F), np.std(list_test_acc_qda_F)))\n",
    "print(\"LDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_lda), np.std(list_test_acc_lda)))\n",
    "print(\"Tensorized LDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_lda_T), np.std(list_test_acc_lda_T)))\n",
    "print(\"Faster LDA acc: {:.3f} (std: {:.3f})\".format(np.mean(list_test_acc_lda_F), np.std(list_test_acc_lda_F)))\n",
    "\n",
    "print(\"QDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_qda), np.std(list_test_time_qda)))\n",
    "print(\"Tensorized QDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_qda_T), np.std(list_test_time_qda_T)))\n",
    "print(\"faster QDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_qda_f), np.std(list_test_time_qda_f)))\n",
    "print(\"Faster QDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_qda_F), np.std(list_test_time_qda_F)))\n",
    "print(\"LDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_lda), np.std(list_test_time_lda)))\n",
    "print(\"Tensorized LDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_lda_T), np.std(list_test_time_lda_T)))\n",
    "print(\"Faster LDA time: {:.5f} (std: {:.5f})\".format(np.mean(list_test_time_lda_F), np.std(list_test_time_lda_F)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuestas**\n",
    "1. En la clase `QDA` (y `TensorizedQDA`), se utiliza un ciclo `for` para recorrer el set a predecir, elemento a elemento. Para cada observación de este set, el método `predict(X)` invoca, para cada elemento del dataset, al método privado `_predict_one(x)` (que a su vez invoca a `_predict_log_conditional(x)`). En particular, para `QDA`, el método `_predict_one(x)` utiliza un ciclo `for` que recorre cada una de las clases (de flores, por ejemplo), mientras que `TensorizedQDA`, lo realiza en una sola pasada cara cada observación particular.  Por medio de tensores, es posible mejorar esta última implementación, de forma de eliminar el ciclo `for` de `predict(X)`.\n",
    "\n",
    "Acá, el set a predecir es $X \\in \\mathbb{R}^{p \\times n}$. Luego, en la implementación, se expande a un tensor $X_{\\text{expanded}} \\in \\mathbb{R}^{clases \\times p \\times n}$ (en este caso, básicamente un tensor formado por 3 matrices de observaciones). Esto permite que las operaciones se lleven a cabo sin utilizar ciclos.\n",
    "\n",
    "A su vez, los promedios están guardados en $\\text{tensor\\_means} \\in \\mathbb{R}^{clases \\times p \\times 1}$, que tambien se expande para tener la dimensión del tensor $X_{\\text{expanded}}$ (para poder operar).\n",
    "\n",
    "Se calcula el tensor de $\\text{unbiased\\_X}$ y su versión transpuesta. Con esto, y con el tensor de las matrices de covarianzas invertidas, se puede calcular el producto interno necesario para hallar el logaritmo de la verosimilitud (en particular, el término $- \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)$). Este producto interno es un tensor de matrices ($n \\times n$). \n",
    "\n",
    "En el código, esta linea es: \n",
    "\n",
    "`inner_prod_mat = unbiased_X_transposed @ self.tensor_inv_cov @ unbiased_X # Forma (classes, n, n)`\n",
    "\n",
    "Dado que lo que interesa es unicamente la diagonal de las matrices de los productos internos, se aplica el método `np.dagonal()`. Con esto se reduce el orden de los tensores ($clases \\times n$)). Para ser consistente con las operaciones y tener un resultado matricial, se calcula el determinante de la matriz de covarianza invertida para cada clase y se lo *broadcastea* a los $n$ elementos del set. Con esto se puede calcular el término faltante para el logaritmo de la verosimilitud, es decir, $\\frac{1}{2} \\log{|\\Sigma_j^{-1}|}$. Esto resulta en una verosimilitud por observación para cada tipo de clase, o lo que es lo mismo, una matriz de $clases \\times n$).\n",
    "\n",
    "Luego, por medio del método `predict(X)` de esta `class`, que a su vez invoca a `_predict_one(X)` se puede obtener la predicción final en una sola pasado, dado que ambos métodos utilizan matrices.\n",
    "\n",
    "2. A este método, que pasa por las matrices de $n \\times n$, se lo llama `fasterQDA`. A continuación puede verse el resultado comparativo de `QDA` (sin usar tensores y con dos ciclos `for`, uno para las clases y el otro para los elementos del set), `TensorizedQDA` (utiliza tensores pero pasa por un ciclo `for` para predecir) y `fasterQDA`. El tiempo se expresa en segundos (set de test, con split de 0.3).\n",
    "\n",
    " - QDA time: 0.00242 (std: 0.00055)\n",
    "\n",
    " - Tensorized QDA time: 0.00062 (std: 0.00051)\n",
    "\n",
    " - faster QDA time: 0.00010 (std: 0.00030)\n",
    "\n",
    "Evidentemente se logran aceleraciones muy altas al pasar de uno a otro. \n",
    "\n",
    "4. Por construcción, los elementos de la diagonal de $A \\cdot B$ están dados por $diag(A \\cdot B)_i = \\sum_{j=1}^{p} a_{ij} \\cdot b_{ji}$, donde $i$ es el índice de la diagonal.\n",
    "\n",
    "Además, $(A \\odot B^T)_{ij} = a_{ij} \\cdot b_{ji}$. Por lo tanto, por comparación, podemos ver que sumando sobre el índice $j$, desde $1$ hasta $p$ (columnas de $A \\odot B^T$), obtenemos que $\\sum_{col}(A \\odot B^T)_i = \\sum_{j=1}^{p} a_{ij} \\cdot b_{ij} = diag(A \\cdot B)_i$.\n",
    "\n",
    "Con esto se ve que $diag(A \\cdot B) = \\sum_{col}(A \\odot B^T)$.\n",
    "\n",
    "Su implementación en **Python**, usando un **NumPy-Array**, donde una matriz se construye como un arreglo de vectores filas, es:\n",
    "$$\n",
    "\\text{np.sum}(A \\odot B^T, \\text{axis}=1)\n",
    "$$\n",
    "En este caso, $\\text{axis}=1$ representa el segundo eje de la matriz, es decir, se suma sobre cada elemento (columna) de las filas de la misma.\n",
    "\n",
    "5. Las diferencias en tiempos encontradas son (set de test, con split de 0.3):\n",
    "\n",
    " - faster QDA time: 0.00010 s (std: 0.00030 s)\n",
    " - Faster QDA time: 0.00008 s (std: 0.00027 s)\n",
    "\n",
    "Diferencias en las implementaciones: \n",
    "\n",
    " - `fasterQDA`:\n",
    "\n",
    "`unbiased_X_transposed =  unbiased_X.transpose(0,2,1) # Forma (classes, n, p)`\n",
    "\n",
    "`inner_prod_mat = unbiased_X_transposed @ self.tensor_inv_cov @ unbiased_X # Forma (classes, n, n)`\n",
    "\n",
    "Multiplico $classes = 3$ matrices de $ n \\times p$ por $classes = 3$ de $p \\times p$, y luego mutiplico por $classes = 3$ matrices de $ p \\times n$. \n",
    "\n",
    "Costo en operaciones matemáticas (multipliaciones): \n",
    " - Primera multiplicación (omitiendo $classes$): $n \\times p^2$\n",
    " - Segunda multiplicación (omitiendo $classes$): $n^2 \\times p$\n",
    "\n",
    " - Total: $classes \\times (n \\times p^2 + n^2 \\times p)$\n",
    "\n",
    " - `FasterQDA`:\n",
    " \n",
    "`unbiased_X_transposed =  unbiased_X.transpose(0,2,1) # Forma (classes, n, p)`\n",
    "\n",
    "`A = unbiased_X_transposed @ self.tensor_inv_cov # Forma (classes, n, p)`\n",
    "\n",
    "`inner_prod = (A * unbiased_X_transposed).sum(axis=-1)  # Forma (classes, n)`\n",
    "\n",
    "Multiplico $classes = 3$ matrices de $ n \\times p$ por $classes = 3$ de $p \\times p$, y luego a esto lo multiplico elemento a elemento por $classes = 3$ matrices de la misma dimensión ($n \\times p$).\n",
    "\n",
    "Costo en operaciones matemáticas (multipliaciones): \n",
    " - Primera multiplicación (omitiendo $classes$): $n \\times p^2$\n",
    " - Segunda multiplicación elemento a elemento (omitiendo $classes$): $n \\times p$\n",
    "\n",
    " - Total: $classes \\times (n \\times p^2 + n \\times p)$\n",
    "\n",
    " Si se tiene en cuenta únicamente el cósto matemátcio de estas lineas de código (las que determinan si se trata de una u otra implemntación), puede verse que la relación entre ellos es $\\frac{classes \\times (n \\times p^2 + n^2 \\times p)}{classes \\times (n \\times p^2 + n \\times p)}$. Considerando el set de test con $n = 45$, $p = 4$, esta relación nos da $\\approx 10$. Es decir, en base a esto sería esperable que `FasterQDA` sea $10$ veces más rápido que `fasterQDA` (suponiendo que el tiempo de cómputo y el número de operaciones es proporcional, aunque en la práctica depende de la arquitectura de hardware, optimizaciones de las bibliotecas, etc). Evidentemente, el costo total tiene en cuenta operaciones adicionales, tales como broadcasting, sumas, transposiciones, multiplicaciones, la búsqueda de los máximos (con `np.maxarg()`), etc. Esto hace que la relación entre el número de operaciones, y por lo tanto consumo de tiempo (que es otra métrica) entre las implementaciones, sea menor (sumamos las mismas cantidades en el numerador y el denominador, lo cual reduce la relación).\n",
    "\n",
    " Además, de la relación mostrada, se ve que aumentando el $n$ (datos a predecir) la relación en el costo matemático debería aumentar. Esto efectivamente ocurre. Por ejemplo, los tiempos de predicción (de nuevo, no es la misma métrica que la de operaciones matemáticas, pero debería ser una buen indicador) considerando el set completo $\\text{X\\_full}$ son:\n",
    "\n",
    " - faster QDA time: 0.00019 s (std: 0.00039 s)\n",
    " - Faster QDA time: 0.00011 s (std: 0.00032 s)\n",
    "\n",
    "En resumen, al evitar el paso por la matriz $n \\times n$, `FasterQDA` logra speedup significativ. Aunque el límite teórico (si la complejidad de la predicción dependiera sólo del cálculo del producto interno) muestra un speedup de $\\frac{n^2 \\times p}{n \\times p^2} = \\frac{n}{p}$ para $n \\gg p$ (teniendo en cuenta la relación de costos mostrada), en la práctica, el costo adicional de otras operaciones reduce este speedup, pero hay que notar que el mismo sigue siendo considerable. El speedup alcanzado en estas implementaciones puede verse a continuación:\n",
    "\n",
    "<img src=\"speedup_vs_n.png\" alt=\"Gráfico de Speedup vs. n\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para evaluar. Demora varios minutos para ejecutarse.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variables para almacenar tamaños y aceleración\n",
    "sizes = []\n",
    "accelerations = []\n",
    "\n",
    "# Incrementar tamaño del conjunto de prueba\n",
    "test_x = X_full.T\n",
    "for i in range(0, 100):  # 100 pasos para aumentar el tamaño de test_x\n",
    "    test_x = np.hstack([test_x, X_full.T])  # Ampliar concatenando\n",
    "    sizes.append(test_x.shape[1])\n",
    "\n",
    "    # Variables para almacenar tiempos de las 10 ejecuciones\n",
    "    time_FasterQDA_list = []\n",
    "    time_fasterQDA_list = []\n",
    "\n",
    "    # Ejecutar 20 veces y almacenar tiempos\n",
    "    for _ in range(20):\n",
    "        # Medir tiempo de FasterQDA\n",
    "        start_time = time()\n",
    "        qda_F.predict(test_x)\n",
    "        time_FasterQDA_list.append(time() - start_time)\n",
    "\n",
    "        # Medir tiempo de fasterQDA\n",
    "        start_time = time()\n",
    "        qda_f.predict(test_x)\n",
    "        time_fasterQDA_list.append(time() - start_time)\n",
    "\n",
    "    # Promedio de tiempos\n",
    "    time_FasterQDA_avg = np.mean(time_FasterQDA_list)\n",
    "    time_fasterQDA_avg = np.mean(time_fasterQDA_list)\n",
    "\n",
    "    acceleration = time_fasterQDA_avg / time_FasterQDA_avg\n",
    "\n",
    "    accelerations.append(acceleration)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descomentar para generar gráfico del speedup\n",
    "\n",
    "# Escalamiento teórico calculado n/p. Dado a que es mayor, uso uno más chico n/100\n",
    "scaling_theoretical_100 = np.array(sizes) / 200\n",
    "scaling_theoretical_4 = np.array(sizes) / 4\n",
    "\n",
    "# Graficar resultados\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(sizes, accelerations, label=\"Speedup (fasterQDA/FasterQDA)\", marker=\"o\", color=\"b\")\n",
    "plt.plot(sizes, scaling_theoretical_100, label=\"Speedup (n/200)\", linestyle=\"--\", color=\"r\")\n",
    "plt.plot(sizes, scaling_theoretical_4, label=\"Speedup (n/4)\", linestyle=\"--\", color=\"black\")\n",
    "plt.ylim(1, 5000)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Elementos del set (n)\")\n",
    "plt.ylabel(\"Speedup\")\n",
    "plt.title(\"Speedup vs. n\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"speedup_vs_n.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7oPeQ0z-pux"
   },
   "source": [
    "## Preguntas teóricas\n",
    "\n",
    "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
    "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
    "3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Partimos de la expresión general para el logaritmo de la función de densidad de probabilidad condicional:\n",
    "\n",
    "$$\\log{f_j(x)} = -\\frac{1}{2} \\log{|\\Sigma|} - \\frac{1}{2} (x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j) + C$$\n",
    "\n",
    "Donde $\\Sigma$ es la matriz de covarianza común a todas las clases en LDA.\n",
    "\n",
    "Entonces, los pasos para llegar a la expresión final son los siguientes:\n",
    "\n",
    "Expandimos el término cuadrático:\n",
    "$$(x - \\mu_j)^T \\Sigma^{-1} (x - \\mu_j) = x^T \\Sigma^{-1} x - x^T \\Sigma^{-1} \\mu_j - \\mu_j^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} \\mu_j$$\n",
    "\n",
    "Simplificamos, notando que $x^T \\Sigma^{-1} \\mu_j$ es un escalar y por lo tanto igual a su transpuesta:\n",
    "$$= x^T \\Sigma^{-1} x - 2 \\mu_j^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} \\mu_j$$\n",
    "\n",
    "Sustituimos esto en la expresión original:\n",
    "$$\\log{f_j(x)} = -\\frac{1}{2} \\log{|\\Sigma|} - \\frac{1}{2} (x^T \\Sigma^{-1} x - 2 \\mu_j^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} \\mu_j) + C$$\n",
    "\n",
    "Distribuimos el $-\\frac{1}{2}$:\n",
    "$$ -\\frac{1}{2} \\log{|\\Sigma|} - \\frac{1}{2} x^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C$$\n",
    "\n",
    "Agrupamos los términos que no dependen de j en una nueva constante C':\n",
    "$$ \\mu_j^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_j^T \\Sigma^{-1} \\mu_j + C'$$\n",
    "\n",
    "Factorizamos $\\mu_j^T \\Sigma^{-1}$:\n",
    "$$ \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j) + C'$$\n",
    "\n",
    "Así llegamos a la expresión final:\n",
    "$$ \\log{f_j(x)} = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j) + C'$$\n",
    "\n",
    "2. En QDA, la función a maximizar es:\n",
    "$$ \\log{f_j(x)} = -\\frac{1}{2} \\log{|\\Sigma_j|} - \\frac{1}{2} (x - \\mu_j)^T \\Sigma_j^{-1} (x - \\mu_j) + C$$\n",
    "\n",
    "Al expandir el término cuadrático, obtenemos términos de segundo orden en x $(x^T \\Sigma_j^{-1} x)$.\n",
    "- Estos términos cuadráticos resultan en límites de decisión cuadráticos en el espacio de características.\n",
    "- La superficie de decisión entre dos clases será una función cuadrática de x.\n",
    "\n",
    "En LDA, la función a maximizar se simplifica a:\n",
    "$$\\log{f_j(x)} = \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j) + C'$$\n",
    "- Esta expresión es lineal en x, ya que solo contiene términos de primer orden en x.\n",
    "- No hay términos cuadráticos en x.\n",
    "- La superficie de decisión entre dos clases será una función lineal de x, resultando en límites de decisión lineales en el espacio de características.\n",
    "\n",
    "3. La implementación de QDA usa:\n",
    "`0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`\n",
    "\n",
    "Mientras que la expresión teórica es:\n",
    "$$-\\frac{1}{2} \\log{|\\Sigma_j|} - \\frac{1}{2} (x - \\mu_j)^T \\Sigma_j^{-1} (x - \\mu_j)$$\n",
    "\n",
    "Diferencias y equivalencias:\n",
    "- Uso de la inversa de la covarianza: La implementación usa inv_cov directamente. Esto es equivalente porque $\\det(\\Sigma_j^{-1}) = 1 / \\det(\\Sigma_j)$.\n",
    "\n",
    "- Signo del término logarítmico: La implementación usa `0.5*np.log(det(inv_cov)) en lugar de $-\\frac{1}{2} \\log{|\\Sigma_j|}$. Esto es equivalente debido a la propiedad del logaritmo: $\\log{(1/x)} = -\\log{(x)}$.\n",
    "\n",
    "- Cálculo del término cuadrático: La implementación usa `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`, donde `unbiased_x` = $(x - \\mu_j)$ Esto es algebraicamente igual a decir que: $(x - \\mu_j)^T \\Sigma_j^{-1} (x - \\mu_j)$.\n",
    "\n",
    "- La implementación no incluye una constante aditiva.\n",
    "Esto no afecta la clasificación, ya que la constante es la misma para todas las clases y no influye en la comparación de probabilidades. Las expresiones son matemáticamente equivalentes, con la implementación utilizando algunas propiedades algebraicas para simplificar el cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpOoxE2d-mOY"
   },
   "source": [
    "## Ejercicio teórico\n",
    "\n",
    "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
    "$$\n",
    "w^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix},\n",
    "b^{(1)} = \\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix},\n",
    "w^{(2)} =\n",
    "\\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix},\n",
    "b^{(2)} = 0.7\n",
    "$$\n",
    "\n",
    "y donde cada capa calcula su salida vía\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
    "$$\n",
    "\n",
    "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
    "\n",
    "\\\\\n",
    "Dada la observación $x=\\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
    "\n",
    "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros iniciales\n",
    "\n",
    "- Pesos de la primera capa:\n",
    "  $$\n",
    "  w^{(1)} = \\begin{pmatrix}\n",
    "  0.1 & -0.5 \\\\\n",
    "  -0.3 & -0.9 \\\\\n",
    "  0.8 & 0.02\n",
    "  \\end{pmatrix}\n",
    "  $$\n",
    "\n",
    "- Sesgos de la primera capa:\n",
    "  $$\n",
    "  b^{(1)} = \\begin{pmatrix}\n",
    "  0.1 \\\\\n",
    "  0.5 \\\\\n",
    "  0.8\n",
    "  \\end{pmatrix}\n",
    "  $$\n",
    "\n",
    "- Pesos de la segunda capa:\n",
    "  $$\n",
    "  w^{(2)} = \\begin{pmatrix}\n",
    "  -0.4 & 0.2 & -0.5\n",
    "  \\end{pmatrix}\n",
    "  $$\n",
    "\n",
    "- Sesgo de la segunda capa:\n",
    "  $$\n",
    "  b^{(2)} = 0.7\n",
    "  $$\n",
    "\n",
    "### Observación y función de costo\n",
    "\n",
    "- Entrada:\n",
    "$$\n",
    "x = \\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- Salida esperada:\n",
    "$$\n",
    "y = 5\n",
    "$$\n",
    "\n",
    "- Función de costo:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}(\\hat{y}_\\theta - y)^2\n",
    "$$\n",
    "\n",
    "### Paso 1: Forward propagation\n",
    "\n",
    "1. Cálculo del input para la primera capa:\n",
    "   $$\n",
    "   z^{(1)} = w^{(1)} x + b^{(1)}\n",
    "   $$\n",
    "\n",
    "   Calculamos cada componente:\n",
    "   - Para la neurona 1:\n",
    "     $$\n",
    "     z_1^{(1)} = (0.1)(1.8) + (-0.5)(-3.4) + 0.1 = 1.98\n",
    "     $$\n",
    "   - Para la neurona 2:\n",
    "     $$\n",
    "     z_2^{(1)} = (-0.3)(1.8) + (-0.9)(-3.4) + 0.5 = 3.02\n",
    "     $$\n",
    "   - Para la neurona 3:\n",
    "     $$\n",
    "     z_3^{(1)} = (0.8)(1.8) + (0.02)(-3.4) + 0.8 = 2.192\n",
    "     $$\n",
    "\n",
    "2. Aplicar la función de activación (sigmoide):\n",
    "   $$\n",
    "   a^{(1)}_i = \\sigma(z_i^{(1)})\n",
    "   $$\n",
    "   Calculamos las salidas:\n",
    "   $$ a_1^{(1)} = \\sigma(1.98) \\approx 0.8787 $$\n",
    "   $$ a_2^{(1)} = \\sigma(3.02) \\approx 0.9536 $$\n",
    "   $$ a_3^{(1)} = \\sigma(2.192) \\approx 0.8997 $$\n",
    "\n",
    "3. Cálculo de la función de preactivación:\n",
    "   $$\n",
    "   z^{(2)} = w^{(2)} a^{(1)} + b^{(2)}\n",
    "   $$\n",
    "\n",
    "   Sustituyendo los valores:\n",
    "   $$\n",
    "   z^{(2)} = (-0.4)(a_1^{(1)}) + (0.2)(a_2^{(1)}) + (-0.5)(a_3^{(1)}) + 0.7\n",
    "   $$\n",
    "   Calculamos:\n",
    "   $$\n",
    "   z^{(2)} \\approx -0.3515 + 0.1907 - 0.44985 + 0.7 \\approx 0.08905\n",
    "   $$\n",
    "\n",
    "4. Salida de la red:\n",
    "   Aplicamos sigmoide nuevamente:\n",
    "   $$\n",
    "   y_{\\hat{\\theta}} = \\sigma(z^{(2)}) \\approx 0.52225\n",
    "   $$\n",
    "\n",
    "### Paso 2: Cálculo del costo y de las derivadas\n",
    "\n",
    "Calculamos la función de costo $J(\\theta)$ = $\\frac{1}{2}(\\hat{y}_\\theta - y)^2$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}(0.522577 - 5)^2 \\approx 10.0432\n",
    "$$\n",
    "\n",
    "La derivada de la función de costo con respecto a la salida es:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial y_{\\hat{\\theta}}} = y_{\\hat{\\theta}} - y\n",
    "$$\n",
    "\n",
    "Sustituyendo los valores obtenidos:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial y_{\\hat{\\theta}}} = (0.52225 - 5) \\approx -4.47775\n",
    "$$\n",
    "\n",
    "### Paso 3: Backpropagation\n",
    "\n",
    "#### Calculo de la derivada de la función de costo, usando la regla de la cadena\n",
    "\n",
    "Para los parámetros de la segunda capa:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i^{(2)}} = \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\hat{y}_\\theta}{\\partial w_i^{(2)}} = \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\hat{y}_\\theta}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial w_i^{(2)}} = \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\sigma(z^{(2)})}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial w_i^{(2)}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\hat{y}_\\theta} = (\\hat{y}_\\theta - y)\n",
    "$$\n",
    "$$\n",
    " \\frac{\\partial \\sigma(z^{(l)})}{\\partial z^{(l)}} = \\frac{\\partial}{\\partial z^{(l)}}\\left( \\frac{1}{1+e^{-z^{(l)}}} \\right) = \\frac{e^{-z^{(l)}}}{(1+e^{-z^{(l)}})^2} = \\frac{1(1-1+e^{-z^{(l)}})}{(1+e^{-z^{(l)}})^2} =\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{1+e^{-z^{(2)}}}\\frac{1-1+e^{-z^{(2)}}}{1+e^{-z^{(2)}}} = \\frac{1}{1+e^{-z^{(2)}}}\\left(\\frac{1+e^{-z^{(2)}}}{1+e^{-z^{(2)}}}-\\frac{1}{1-e^{-z^{(2)}}}\\right) =\n",
    "$$\n",
    "$$\n",
    "= \\sigma(z^{(2)}) (1-\\sigma(z^{(2)}) = \\hat{y}_\\theta (1-\\hat{y}_\\theta)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z^{(2)}}{\\partial w_i^{(2)}} = a_i^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i^{(2)}} =  (\\hat{y}_\\theta - y) \\hat{y}_\\theta (1-\\hat{y}_\\theta) a_i^{(1)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{(2)}} =  \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\hat{y}_\\theta}{\\partial z^{(2)}} \\frac{\\partial z^{(2)}}{\\partial b^{(2)}}  = (\\hat{y}_\\theta - y) \\hat{y}_\\theta (1-\\hat{y}_\\theta) \n",
    "$$\n",
    "\n",
    "Para los parámetros de la primera capa:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(1)}} = \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\hat{y}_\\theta}{\\partial z^{(2)}} \\frac{\\partial z_i^{(2)}}{\\partial a_i^{(1)}} \\frac{\\partial a_i^{(1)}}{\\partial z_i^{(1)}} \\frac{\\partial z_i^{(1)}}{\\partial w_{ij}^{(1)}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\hat{y}_\\theta} = (\\hat{y}_\\theta - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial a^{(2)}} = a^{(2)} - y = 0.522577 - 5 = -4.477423\n",
    "$$\n",
    "\n",
    "Luego:\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_\\theta}{\\partial z^{(2)}} = \\frac{\\partial \\sigma(z^{(2)})}{\\partial z^{(2)}} = \\hat{y}_\\theta (1-\\hat{y}_\\theta)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z_i^{(2)}}{\\partial a_i^{(1)}} = w_i^{(2)}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial a_i^{(1)}}{\\partial z_i^{(1)}} = \\frac{\\partial \\sigma(z_i^{(1)})}{\\partial z_i^{(1)}} = \\sigma(z_i^{(1)})(1-\\sigma(z_i^{(1)})) = a_i^{(1)}(1-a_i^{(1)})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z_i^{(1)}}{\\partial w_{ij}^{(1)}} = a_j^{(0)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ij}^{(1)}} = (\\hat{y}_\\theta - y) \\hat{y}_\\theta (1-\\hat{y}_\\theta) w_i^{(2)} a_i^{(1)}(1-a_i^{(1)}) a_j^{(0)}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_{i}^{(1)}} = \\frac{\\partial J}{\\partial \\hat{y}_\\theta} \\frac{\\partial \\hat{y}_\\theta}{\\partial z^{(2)}} \\frac{\\partial z_i^{(2)}}{\\partial a_i^{(1)}} \\frac{\\partial a_i^{(1)}}{\\partial z_i^{(1)}} \\frac{\\partial z_i^{(1)}}{\\partial b_{i}^{(1)}} = (\\hat{y}_\\theta - y) \\hat{y}_\\theta (1-\\hat{y}_\\theta) w_i^{(2)} a_i^{(1)}(1-a_i^{(1)})\n",
    "$$\n",
    "\n",
    "## Cálculo de las derivadas para cada parámetro de la capa de salida\n",
    "\n",
    "1) Calculamos $\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}$:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} = a^{(2)}(1 - a^{(2)}) = 0.522577(1 - 0.522577) \\approx 0.249498\n",
    "   $$\n",
    "\n",
    "2) Calculamos $\\frac{\\partial J}{\\partial z^{(2)}}$:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial z^{(2)}} = \\frac{\\partial J}{\\partial a^{(2)}} \\cdot \\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\approx -4.477423 \\cdot 0.249498 \\approx -1.117106\n",
    "   $$\n",
    "\n",
    "3) Entonces, la derivada de $J$ respecto a $w^{(2)}$ es:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w^{(2)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\cdot a^{(1)}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w^{(2)}} \\approx [-0.981481, -1.064588, -1.002604]\n",
    "   $$\n",
    "\n",
    "4) La derivada de $J$ respecto a $b^{(2)}$ es:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial b^{(2)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\approx -1.117106\n",
    "   $$\n",
    "\n",
    "5) La derivada de $J$ respecto a $a^{(1)}$ es:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial a^{(1)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\cdot w^{(2)}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial a^{(1)}} \\approx [0.446842, -0.223421, 0.558553]\n",
    "   $$\n",
    "\n",
    "6) La derivada de $J$ respecto a $z^{(1)}$ es:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial z^{(1)}} = \\frac{\\partial J}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial z^{(1)}} \\approx [0.047735, -0.009935, 0.051354]\n",
    "   $$\n",
    "\n",
    "7) La derivada de $J$ respecto para $w^{(1)}$ es:\n",
    "   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w^{(1)}} = \\frac{\\partial J}{\\partial z^{(1)}} \\cdot x^T\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w^{(1)}} \\approx \\begin{bmatrix}\n",
    "   0.085923 & -0.162299 \\\\\n",
    "   -0.017883 & 0.033779 \\\\\n",
    "   0.092437 & -0.174604\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   \n",
    "9) La derivada de $J$ respecto a $b^{(1)}$ es:   \n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial b^{(1)}} = \\frac{\\partial J}{\\partial z^{(1)}} \\approx [0.047735, -0.009935, 0.051354]\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-CNlW06-i-u"
   },
   "source": [
    "## Preguntas en el código\n",
    "Previamente las preguntas \"técnicas\" en comentarios en el código eran parte del TP, y buscaban que el alumno logre entrar en el detalle de por qué cada linea de código es como es y en el orden en el que está. Ya no forman parte de la consigna, pero se aconseja al alumno intentar responderlas. Las respuestas a las mismas se encuentran en un archivo separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassEncoder\n",
    "\n",
    "Q1: ¿Por qué no hace falta definir un `class_to_name` para el mapeo inverso?\n",
    "- Porque el mapeo de clase a nombre (`int` a `string`) ya se puede accediendo al índice correspondiente del vector `self.names`.\n",
    "\n",
    "Q2: ¿Por qué hace falta un `reshape`?\n",
    "- Porque como **NumPy** no tiene una función que aplique elemento a elemento una función escalar, la aplicación se debe hacer vía una list comprehension sobre el arreglo flatteneado. Al hacer el `flatten` se pierde la dimensionalidad original, por lo que es necesario un `reshape` para recuperarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseBayesianClassifier\n",
    "\n",
    "Q3: ¿Para qué sirve `bincount`?\n",
    "\n",
    "- Sirve para contar las frecuencias absolutas de aparición de cada clase. Al dividir por `y.size`, se recuperan las frecuencias relativas, que son la estimación de las probabilidades a priori de cada clase.\n",
    "\n",
    "Q4: ¿Por qué el `_fit_params` va al final? ¿No se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "Repasemos qué ocurre antes:\n",
    "- Se encodean las clases para pasarlas a valores numéricos entre $0$ y $k-1$.\n",
    "- Se obtienen las probabilidades a priori (impuestas o estimadas). Notar que si no se encodean previamente las clases, el `bincount` de `_estimate_a_priori` se rompe.\n",
    "\n",
    "- El método `_fit_params` requiere que las clases estén encodeadas y utiliza el atributo `self.log_a_priori`, por lo que rompe si alguno de los anteriores no es realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA\n",
    "\n",
    "Q5: ¿Por qué hace falta el `flatten` y no se puede directamente `X[:,y==idx]`?\n",
    "- Porque `y` en realidad está modelado como una matriz para poder ser un vector columna, y **NumPy** arroja error si se intenta indexar una dimensión (la de las columnas de $X$) con un array de dos dimensiones, incluso si una de esas dos es de tamaño $1$.\n",
    "\n",
    "Q6: ¿Por qué se usa `bias=True` en vez del default `bias=False`?\n",
    "- Porque `bias = True` divide por $n$ en vez de por $n-1$ (insesgada), que es el valor default, y es lo que queremos porque ese es el estimador de máxima verosimilitud.\n",
    "\n",
    "Q7: ¿Qué hace `axis = 1`? ¿Por qué no $axis = 0`?\n",
    "- Porque en nuestro esquema de observaciones como vectores columna, los datos se encuentran en formato (features, observaciones), donde una matriz $X$ de dimensiones $(p, n)$ indica que tiene $p$ features y hay $n$ observaciones. Calcular la media sobre las columnas es promediar sobre las observaciones, hacerlo sobre las features ($axis=0$) no tendría sentido."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
